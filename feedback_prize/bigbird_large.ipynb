{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigBird Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Loading Libraries\n",
      "=======Versions=======\n",
      "Python: 3.8.0 (default, Jan 20 2022, 17:32:02) \n",
      "[Clang 13.0.0 (clang-1300.0.27.3)]\n",
      "NumPy: 1.22.1\n",
      "PyTorch: 1.10.2\n",
      "Transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "import os, sys, tqdm, sklearn, sklearn.metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch, torch.utils\n",
    "import transformers\n",
    "\n",
    "import utils.preprocessing as preprocessing\n",
    "import utils.dataset as dataset\n",
    "import utils.metrics as metrics\n",
    "\n",
    "print(\"Done Loading Libraries\")\n",
    "print(\"Versions\".center(22, \"=\"))\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/bigbird-roberta-large\"\n",
    "MODEL_DOWNLOAD = False\n",
    "MODEL_PATH = \"./models/bigbird-roberta-large\"\n",
    "\n",
    "DATA_DAWNLOAD = False\n",
    "\n",
    "VERSION = \"001\"\n",
    "\n",
    "config = {'model_name': MODEL_NAME, 'max_length': 1024, 'train_batch_size':4, 'valid_batch_size':4, 'epochs':5, 'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7], 'max_grad_norm':10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Load a Modal!!\n"
     ]
    }
   ],
   "source": [
    "if MODEL_DOWNLOAD:\n",
    "    os.makedirs(\"models/bigbird-roberta-large\", exist_ok=True)\n",
    "\n",
    "    # tokenizerのダウンロード\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "    tokenizer.save_pretrained(\"models/bigbird-roberta-large\")\n",
    "\n",
    "    # configのダウンロード\n",
    "    config_model = transformers.AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    config_model.num_labels = 15\n",
    "    config_model.save_pretrained(\"models/bigbird-roberta-large\")\n",
    "\n",
    "    # model weightのダウンロード\n",
    "    backbone = transformers.AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config_model)\n",
    "    backbone.save_pretrained(\"models/bigbird-roberta-large\")\n",
    "\n",
    "    print(\"Done download!!\")\n",
    "else:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    config_model = transformers.AutoConfig.from_pretrained(MODEL_PATH + \"/config.json\")\n",
    "    model = transformers.AutoModelForTokenClassification.from_pretrained(MODEL_PATH + \"/pytorch_model.bin\", config=config_model)\n",
    "    print(\"Done Load a Modal!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Datasetの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_DAWNLOAD:\n",
    "    preprocessing.make_NER_dataframe(\"./data\", n_splits=10)\n",
    "else:\n",
    "    train_df, val_df, lookups = preprocessing.read_kfold_file(\"./data\", fold=7, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.CustomDataset(train_df, lookups, tokenizer, max_len=config[\"max_length\"], val=False)\n",
    "val_dataset = dataset.CustomDataset(train_df, lookups, tokenizer, max_len=config[\"max_length\"], val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': config['train_batch_size'], 'shuffle': True, 'num_workers': 2, 'pin_memory':True}\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, **train_params)\n",
    "\n",
    "val_params = {'batch_size': config['valid_batch_size'], 'shuffle': False, 'num_workers': 2, 'pin_memory':True}\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, **val_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trainmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, config):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device:\", device)\n",
    "    model.to(device)\n",
    "\n",
    "    logs = []\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        iters = 0\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = config[\"learning_rates\"][epoch]\n",
    "\n",
    "        phase = \"train\"\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        with tqdm.tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{config['epochs']}\") as t:\n",
    "            for batch in t:\n",
    "                iters += 1\n",
    "\n",
    "                ids = batch[\"input_ids\"].to(device, dtype = torch.long)\n",
    "                mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "                labels = batch[\"labels\"].to(device, dtype=torch.long)\n",
    "\n",
    "                loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=False)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Accuracyの計算\n",
    "                flatted_labels = labels.view(-1)    # [batch_size * seq_len, ]\n",
    "                active_logits = tr_logits.view(-1, model.num_labels)    # [batch_size * seqlen, num_labels]\n",
    "                flatted_predictions = torch.argmax(active_logits, axis=1)   # [batch_size * seq_len, ]\n",
    "\n",
    "                # only compute accuracy at active labels\n",
    "                active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "\n",
    "                labels = torch.masked_select(flatted_labels, active_accuracy)\n",
    "                predictions = torch.masked_select(flatted_predictions, active_accuracy)\n",
    "\n",
    "                tr_acc = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "                epoch_acc += tr_acc\n",
    "\n",
    "                # バックプロパゲーション\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # 勾配クリッピング(Normで)\n",
    "                torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config[\"max_grad_norm\"])\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                t.set_postfix_str(f\"Loss: {epoch_loss/iters:.4f}, Acc: {epoch_acc/iters:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        logs.append({\"epoch\": epoch + 1, \"train_loss\": epoch_loss / iters, \"train_acc\": epoch_acc / iters})\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(f\"./logs/log_bigbird-roberta-large_001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config[\"learning_rates\"][0])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "train_model(model, train_dataloader, optimizer, config, config[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH + f\"/bigbird-roberta-large_{VERSION}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47895a868a908f95bd126b96f1c0697fa3ee0581f0f761f9e57a5f9573e4b086"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
